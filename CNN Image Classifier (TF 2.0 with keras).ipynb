{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n",
    "from random import shuffle, randint, seed\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import argparse\n",
    "from tensorflow.keras.utils import get_file\n",
    "from ffhq_dataset.face_alignment import image_align\n",
    "from ffhq_dataset.landmarks_detector import LandmarksDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "OpenCV version: 4.1.0",
      "\n",
      "Tensorflow version: 2.0.0",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(f'OpenCV version: {cv2.__version__}')\n",
    "print(f'Tensorflow version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Gets the repo for aligned images\n",
    "\n",
    "# !rm -rf sample_data\n",
    "# !git clone https://github.com/pbaylies/stylegan-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Check what folder to use for training and testing images\n",
    "CHISOM_TRAIN_DIR = 'C:/Users/chiso/MEGA/data/train'\n",
    "CHISOM_TEST_DIR = 'C:/Users/chiso/MEGA/data/test'\n",
    "CHISOM_ALIGNED_TRAIN_DIR = 'C:/Users/chiso/MEGA/data/aligned_train'\n",
    "CHISOM_ALIGNED_TEST_DIR = 'C:/Users/chiso/MEGA/data/aligned_test'\n",
    "\n",
    "YISI_TRAIN_DIR = 'E:/MegaSync/data/train'\n",
    "YISI_TEST_DIR = 'E:/MegaSync/data/test'\n",
    "YISI_ALIGNED_TRAIN_DIR = 'E:/MegaSync/data/aligned_train'\n",
    "YISI_ALIGNED_TEST_DIR = 'E:/MegaSync/data/aligned_test'\n",
    "\n",
    "\n",
    "def get_directories():\n",
    "    if os.path.exists(CHISOM_TRAIN_DIR) and os.path.exists(CHISOM_TEST_DIR) \\\n",
    "            and os.path.exists(CHISOM_ALIGNED_TRAIN_DIR) and os.path.exists(CHISOM_ALIGNED_TEST_DIR):\n",
    "        return CHISOM_TRAIN_DIR, CHISOM_TEST_DIR, CHISOM_ALIGNED_TRAIN_DIR, CHISOM_ALIGNED_TEST_DIR\n",
    "    else:\n",
    "        return YISI_TRAIN_DIR, YISI_TEST_DIR, YISI_ALIGNED_TRAIN_DIR, YISI_ALIGNED_TEST_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "38"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 106
    }
   ],
   "source": [
    "DATE = datetime.datetime.now().strftime('%d-%b-%Y')\n",
    "TRAIN_DIR, TEST_DIR, ALIGNED_TRAIN_DIR, ALIGNED_TEST_DIR = get_directories()\n",
    "IMG_SIZE = 50\n",
    "LR = 1e-3\n",
    "MODEL_PATH = f'models/{DATE}/'\n",
    "MODEL_NAME = 'ImageClassifier-keras-5-Conv-Layer-{}.model'.format(int(time.time()))\n",
    "TENSORBOARD = TensorBoard(log_dir=f'logs\\\\{MODEL_NAME}') \n",
    "NUM_CLASSES = len(next(os.walk(ALIGNED_TRAIN_DIR))[1])\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('C:/Users/chiso/MEGA/data/test',\n 'C:/Users/chiso/MEGA/data/train',\n 'C:/Users/chiso/MEGA/data/aligned_train',\n 'C:/Users/chiso/MEGA/data/aligned_test')"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 107
    }
   ],
   "source": [
    "TEST_DIR, TRAIN_DIR, ALIGNED_TRAIN_DIR, ALIGNED_TEST_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walkthrough of Subfolders in Train Directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Roots = C:/Users/chiso/MEGA/data/aligned_train",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Only the root\n",
    "ROOTS = next(os.walk(ALIGNED_TRAIN_DIR))[0]\n",
    "print(f\"Roots = {ROOTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Abella Danger',\n 'Aiden Starr',\n 'Aidra Fox',\n 'Aletta Ocean',\n 'Alina Lopez',\n 'Allie Haze',\n 'Amirah Adara',\n 'Andriana Chechik',\n 'Ariella Ferrera',\n 'Brenda James',\n 'Chastity Lynn',\n 'Dana DeArmond',\n 'Dana Weyron',\n 'Emily Willis',\n 'Evelina Darling',\n 'Jessa Rhodes',\n 'Jessica Bangkok',\n 'Julia Ann',\n 'Kimmy Granger',\n 'Krystal Boyd',\n 'Lana Rhoades',\n 'Leyla Fiore',\n 'Little Caprice',\n 'Madison Ivy',\n 'Marcelin Abadir',\n 'Mellanie Monroe',\n 'Mia Khalifa',\n 'Nicole Aniston',\n 'Peta Jensen',\n 'Riley Reid',\n 'Riley Steele',\n 'Samantha Ryan',\n 'Shyla Jennings',\n 'Stella Cox',\n 'Tanya Tate',\n 'Valentina Nappi',\n 'Xev Bellringer',\n 'Zoey Holloway']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 109
    }
   ],
   "source": [
    "# Only the directories\n",
    "DIRS = next(os.walk(ALIGNED_TRAIN_DIR))[1]\n",
    "DIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"for root, dirs, files in os.walk(ALIGNED_TRAIN_DIR):\\n    for name in files:\\n        print(name.split('.')[0]) # filters the file name by file extension and the copy_number\\n        \\n\""
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 110
    }
   ],
   "source": [
    "# Only the files\n",
    "\"\"\"for root, dirs, files in os.walk(ALIGNED_TRAIN_DIR):\n",
    "    for name in files:\n",
    "        print(name.split('.')[0]) # filters the file name by file extension and the copy_number\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of Image classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'def get_class_labels():\\n    labels = []\\n    for root, dirs, files in os.walk(TRAIN_DIR):\\n        path = root.split(os.sep)\\n        for folder in dirs:\\n            name = folder.split()\\n            class_label = \"\".join([letter[0] for letter in name])\\n            labels.append(class_label)\\n    return labels'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 111
    }
   ],
   "source": [
    "# Used for abbreviating the class names NOT USED\n",
    "\n",
    "\"\"\"def get_class_labels():\n",
    "    labels = []\n",
    "    for root, dirs, files in os.walk(TRAIN_DIR):\n",
    "        path = root.split(os.sep)\n",
    "        for folder in dirs:\n",
    "            name = folder.split()\n",
    "            class_label = \"\".join([letter[0] for letter in name])\n",
    "            labels.append(class_label)\n",
    "    return labels\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "LABELS = next(os.walk(ALIGNED_TRAIN_DIR))[1] # all the class labels (pornstar names) to be used\n",
    "LABELS = np.reshape(LABELS, (-1, 1)) # reshapes array from 1D to 2D array\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoded_labels = np.array(mlb.fit_transform(LABELS))\n",
    "# dict(zip(LABELS.flatten(), encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# img.split('.')[0].split('(')[0]  # filters the file name by file extension and the copy_number\n",
    "\"\"\"\n",
    "Labelled training data\n",
    "\"\"\"\n",
    "def create_train_data():\n",
    "    training_data = []\n",
    "    # iterate over each image-class (subfolder) in training directory\n",
    "    for folder in tqdm(os.listdir(ALIGNED_TRAIN_DIR)):\n",
    "        full_path = f'{ALIGNED_TRAIN_DIR}/{folder}'\n",
    "        # iterate over each image in each subfolder\n",
    "        for img in os.listdir(full_path):\n",
    "            ##### !python align_images.py raw_images/ aligned_images/ --output_size=1048\n",
    "            img_name = str(folder)  # the sub-folder is used as the image name for each image\n",
    "            img_name = img_name.strip() # removes any leading and trailing whitespaces from the img name\n",
    "            label = mlb.transform([[img_name]]) # encodes the label of the image using MultiLabelBinarizer\n",
    "            label = label.flatten()  # converts encoded label from 2D to 1D array\n",
    "            # print(f'Image: {img} - Encoding:{label}')\n",
    "            path = os.path.join(full_path, img)  # full path of the image\n",
    "            # feature extraction\n",
    "            img = cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE))\n",
    "            img = tf.cast(img, tf.float32) # change data type of image to float32\n",
    "            training_data.append([np.array(img), np.array(label)])\n",
    "    shuffle(training_data)\n",
    "    np.save('train_data.npy', training_data)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unlabelled test data\n",
    "\"\"\"\n",
    "def process_test_data():\n",
    "    img_ids = list(range(len(os.listdir(ALIGNED_TEST_DIR)))) # generates list of ID numbers\n",
    "    shuffle(img_ids) # randomly assorted\n",
    "    img_ids = iter(img_ids) \n",
    "    testing_data = [] \n",
    "    for img in tqdm(os.listdir(ALIGNED_TEST_DIR)):\n",
    "        path = os.path.join(ALIGNED_TEST_DIR, img)\n",
    "        img_num = next(img_ids)\n",
    "        print(f\"ID: {img_num} - Image: {img}\")\n",
    "        # feature extraction\n",
    "        img = cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE))\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        testing_data.append([np.array(img), img_num])\n",
    "    np.save('test_data.npy', testing_data)\n",
    "    return testing_data     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generate Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "\r  0%|          | 0/38 [00:00<?, ?it/s]",
      "\r  3%|▎         | 1/38 [00:02<01:23,  2.26s/it]",
      "\r  5%|▌         | 2/38 [00:03<01:14,  2.08s/it]",
      "\r  8%|▊         | 3/38 [00:06<01:17,  2.21s/it]",
      "\r 11%|█         | 4/38 [00:08<01:15,  2.23s/it]",
      "\r 13%|█▎        | 5/38 [00:10<01:07,  2.06s/it]",
      "\r 16%|█▌        | 6/38 [00:13<01:13,  2.29s/it]",
      "\r 18%|█▊        | 7/38 [00:16<01:16,  2.48s/it]",
      "\r 21%|██        | 8/38 [00:18<01:15,  2.53s/it]",
      "\r 24%|██▎       | 9/38 [00:21<01:13,  2.55s/it]",
      "\r 26%|██▋       | 10/38 [00:23<01:06,  2.39s/it]",
      "\r 29%|██▉       | 11/38 [00:25<00:59,  2.21s/it]",
      "\r 32%|███▏      | 12/38 [00:27<01:00,  2.34s/it]",
      "\r 34%|███▍      | 13/38 [00:29<00:56,  2.28s/it]",
      "\r 37%|███▋      | 14/38 [00:32<00:54,  2.26s/it]",
      "\r 39%|███▉      | 15/38 [00:33<00:49,  2.13s/it]",
      "\r 42%|████▏     | 16/38 [00:36<00:48,  2.22s/it]",
      "\r 45%|████▍     | 17/38 [00:38<00:45,  2.15s/it]",
      "\r 47%|████▋     | 18/38 [00:40<00:44,  2.24s/it]",
      "\r 50%|█████     | 19/38 [00:43<00:44,  2.33s/it]",
      "\r 53%|█████▎    | 20/38 [00:45<00:39,  2.17s/it]",
      "\r 55%|█████▌    | 21/38 [00:47<00:36,  2.16s/it]",
      "\r 58%|█████▊    | 22/38 [00:48<00:31,  1.99s/it]",
      "\r 61%|██████    | 23/38 [00:51<00:31,  2.08s/it]",
      "\r 63%|██████▎   | 24/38 [00:53<00:29,  2.11s/it]",
      "\r 66%|██████▌   | 25/38 [00:54<00:25,  1.95s/it]",
      "\r 68%|██████▊   | 26/38 [00:57<00:24,  2.01s/it]",
      "\r 71%|███████   | 27/38 [00:58<00:21,  1.94s/it]",
      "\r 74%|███████▎  | 28/38 [01:00<00:18,  1.83s/it]",
      "\r 76%|███████▋  | 29/38 [01:02<00:17,  1.94s/it]",
      "\r 79%|███████▉  | 30/38 [01:04<00:16,  2.05s/it]",
      "\r 82%|████████▏ | 31/38 [01:06<00:13,  1.94s/it]",
      "\r 84%|████████▍ | 32/38 [01:08<00:12,  2.05s/it]",
      "\r 87%|████████▋ | 33/38 [01:11<00:10,  2.12s/it]",
      "\r 89%|████████▉ | 34/38 [01:12<00:07,  1.95s/it]",
      "\r 92%|█████████▏| 35/38 [01:15<00:06,  2.05s/it]",
      "\r 95%|█████████▍| 36/38 [01:16<00:04,  2.01s/it]",
      "\r 97%|█████████▋| 37/38 [01:18<00:01,  1.79s/it]",
      "\r100%|██████████| 38/38 [01:20<00:00,  1.82s/it]",
      "\r100%|██████████| 38/38 [01:20<00:00,  2.11s/it]",
      "\n",
      "\r  0%|          | 0/24 [00:00<?, ?it/s]",
      "\r 12%|█▎        | 3/24 [00:00<00:00, 24.06it/s]",
      "\r 17%|█▋        | 4/24 [00:00<00:01, 12.79it/s]",
      "\r 25%|██▌       | 6/24 [00:00<00:01, 14.26it/s]",
      "\r 33%|███▎      | 8/24 [00:00<00:01, 13.95it/s]",
      "\r 42%|████▏     | 10/24 [00:00<00:01, 11.34it/s]",
      "\r 50%|█████     | 12/24 [00:00<00:00, 12.41it/s]",
      "\r 58%|█████▊    | 14/24 [00:01<00:00, 12.76it/s]",
      "\r 67%|██████▋   | 16/24 [00:01<00:00,  8.54it/s]",
      "\r 75%|███████▌  | 18/24 [00:01<00:00,  9.87it/s]",
      "\r 83%|████████▎ | 20/24 [00:01<00:00, 11.39it/s]",
      "\r 92%|█████████▏| 22/24 [00:01<00:00, 11.90it/s]",
      "\r100%|██████████| 24/24 [00:01<00:00, 13.15it/s]",
      "\r100%|██████████| 24/24 [00:01<00:00, 12.01it/s]",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "ID: 15 - Image: Aiden Starr.png",
      "\n",
      "ID: 21 - Image: Allie Haze.png",
      "\n",
      "ID: 11 - Image: Andriana Chechik.png",
      "\n",
      "ID: 12 - Image: Anna.jpg",
      "\n",
      "ID: 9 - Image: Ariella Ferrera.png",
      "\n",
      "ID: 5 - Image: Emily Willis.png",
      "\n",
      "ID: 19 - Image: Evelina Darling.png",
      "\n",
      "ID: 1 - Image: Jessa Rhodes.png",
      "\n",
      "ID: 14 - Image: Jessica Bangkok.png",
      "\n",
      "ID: 6 - Image: Julia Ann.png",
      "\n",
      "ID: 18 - Image: Krystal Boyd.png",
      "\n",
      "ID: 22 - Image: Lana Rhoades.png",
      "\n",
      "ID: 20 - Image: Madison Ivy.png",
      "\n",
      "ID: 13 - Image: Marcelin Abadir.png",
      "\n",
      "ID: 16 - Image: Mellanie Monroe.png",
      "\n",
      "ID: 0 - Image: Mia Khalifa.png",
      "\n",
      "ID: 10 - Image: Nicole Aniston.png",
      "\n",
      "ID: 23 - Image: Peta Jensen.png",
      "\n",
      "ID: 4 - Image: Riley Reid.png",
      "\n",
      "ID: 8 - Image: Samantha Ryan.png",
      "\n",
      "ID: 2 - Image: Stella Cox.png",
      "\n",
      "ID: 3 - Image: Tanya Tate.png",
      "\n",
      "ID: 17 - Image: Valentina Nappi.png",
      "\n",
      "ID: 7 - Image: Zoey Holloway.png",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "train_data = create_train_data()\n",
    "test_data = process_test_data()\n",
    "# if train/test data already exists\n",
    "# train_data = np.load('train_data.npy', allow_pickle=True)\n",
    "# test_data = np.load('test_data.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "2401",
      "\n",
      "24",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Convoluted Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    # tf.reset_default_graph()\n",
    "    model = Sequential()\n",
    "    input_shape = (IMG_SIZE, IMG_SIZE, 1)\n",
    "    \n",
    "    # INPUT LAYER\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # HIDDEN LAYER 1\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # HIDDEN LAYER 2\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # HIDDEN LAYER 3\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # HIDDEN LAYER 4\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Fully Connected\n",
    "    model.add(Flatten()) # converts the 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5)) # reduces overfitting\n",
    "\n",
    "    # OUTPUT LAYER\n",
    "    model.add(Dense(NUM_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING THE NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cross Validation Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ratio = int(round(len(train_data), -1) * 0.2)\n",
    "train = train_data[:-ratio] # sample train data\n",
    "test = train_data[-ratio:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_X = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 1) # train features (images)\n",
    "train_Y = np.array([i[1] for i in train]) # train labels\n",
    "\n",
    "test_X = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 1) # test features (images)\n",
    "test_Y = np.array([i[1] for i in test]) # test labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Feature Scaling (Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Have to divide by 255 \n",
    "train_X = train_X/255.0\n",
    "test_X = test_X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "train data: (2041, 50, 50, 1)",
      "\n",
      "train labels: (2041, 38)",
      "\n",
      "test data: (360, 50, 50, 1)",
      "\n",
      "test labels: (360, 38)",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(f\"train data: {train_X.shape}\")\n",
    "print(f\"train labels: {train_Y.shape}\")\n",
    "print(f\"test data: {test_X.shape}\")\n",
    "print(f\"test labels: {test_Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency distribution of classes being used in \"test data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({('Krystal Boyd',): 7,\n         ('Mellanie Monroe',): 10,\n         ('Aidra Fox',): 18,\n         ('Riley Steele',): 6,\n         ('Andriana Chechik',): 15,\n         ('Emily Willis',): 12,\n         ('Samantha Ryan',): 11,\n         ('Jessa Rhodes',): 13,\n         ('Tanya Tate',): 9,\n         ('Chastity Lynn',): 9,\n         ('Allie Haze',): 13,\n         ('Ariella Ferrera',): 16,\n         ('Zoey Holloway',): 9,\n         ('Little Caprice',): 12,\n         ('Mia Khalifa',): 10,\n         ('Aiden Starr',): 4,\n         ('Julia Ann',): 9,\n         ('Nicole Aniston',): 5,\n         ('Dana Weyron',): 13,\n         ('Amirah Adara',): 12,\n         ('Madison Ivy',): 8,\n         ('Leyla Fiore',): 4,\n         ('Kimmy Granger',): 11,\n         ('Aletta Ocean',): 8,\n         ('Lana Rhoades',): 9,\n         ('Abella Danger',): 7,\n         ('Peta Jensen',): 8,\n         ('Riley Reid',): 13,\n         ('Dana DeArmond',): 18,\n         ('Brenda James',): 4,\n         ('Marcelin Abadir',): 6,\n         ('Valentina Nappi',): 9,\n         ('Jessica Bangkok',): 9,\n         ('Shyla Jennings',): 13,\n         ('Xev Bellringer',): 6,\n         ('Evelina Darling',): 4,\n         ('Stella Cox',): 4,\n         ('Alina Lopez',): 6})"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 122
    }
   ],
   "source": [
    "enc = []\n",
    "for img in test:\n",
    "    enc.append(img[1])\n",
    "    \n",
    "enc = np.array(enc)\n",
    "test_labels = mlb.inverse_transform(enc)\n",
    "c = Counter(test_labels)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"",
      "\n",
      "_________________________________________________________________",
      "\n",
      "Layer (type)                 Output Shape              Param #   ",
      "\n",
      "=================================================================",
      "\n",
      "conv2d_16 (Conv2D)           (None, 48, 48, 32)        320       ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "activation_19 (Activation)   (None, 48, 48, 32)        0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "conv2d_17 (Conv2D)           (None, 46, 46, 32)        9248      ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "activation_20 (Activation)   (None, 46, 46, 32)        0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "max_pooling2d_12 (MaxPooling (None, 23, 23, 32)        0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "conv2d_18 (Conv2D)           (None, 21, 21, 64)        18496     ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "activation_21 (Activation)   (None, 21, 21, 64)        0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "max_pooling2d_13 (MaxPooling (None, 10, 10, 64)        0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "dropout_11 (Dropout)         (None, 10, 10, 64)        0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "conv2d_19 (Conv2D)           (None, 8, 8, 64)          36928     ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "activation_22 (Activation)   (None, 8, 8, 64)          0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "max_pooling2d_14 (MaxPooling (None, 4, 4, 64)          0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "dropout_12 (Dropout)         (None, 4, 4, 64)          0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "conv2d_20 (Conv2D)           (None, 2, 2, 64)          36928     ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "activation_23 (Activation)   (None, 2, 2, 64)          0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "max_pooling2d_15 (MaxPooling (None, 1, 1, 64)          0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "dropout_13 (Dropout)         (None, 1, 1, 64)          0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "flatten_2 (Flatten)          (None, 64)                0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "dense_4 (Dense)              (None, 128)               8320      ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "activation_24 (Activation)   (None, 128)               0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "dropout_14 (Dropout)         (None, 128)               0         ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "dense_5 (Dense)              (None, 38)                4902      ",
      "\n",
      "_________________________________________________________________",
      "\n",
      "activation_25 (Activation)   (None, 38)                0         ",
      "\n",
      "=================================================================",
      "\n",
      "Total params: 115,142",
      "\n",
      "Trainable params: 115,142",
      "\n",
      "Non-trainable params: 0",
      "\n",
      "_________________________________________________________________",
      "\n",
      "Train on 2041 samples, validate on 360 samples",
      "\n",
      "Epoch 1/150",
      "\n",
      "2041/2041 - 13s - loss: 3.6372 - accuracy: 0.0230 - val_loss: 3.6294 - val_accuracy: 0.0250\n",
      "Epoch 2/150",
      "\n",
      "2041/2041 - 12s - loss: 3.6292 - accuracy: 0.0392 - val_loss: 3.6226 - val_accuracy: 0.0500\n",
      "Epoch 3/150",
      "\n",
      "2041/2041 - 14s - loss: 3.6247 - accuracy: 0.0323 - val_loss: 3.6110 - val_accuracy: 0.0500\n",
      "Epoch 4/150",
      "\n",
      "2041/2041 - 12s - loss: 3.6203 - accuracy: 0.0372 - val_loss: 3.6136 - val_accuracy: 0.0361\n",
      "Epoch 5/150",
      "\n",
      "2041/2041 - 13s - loss: 3.6188 - accuracy: 0.0333 - val_loss: 3.5878 - val_accuracy: 0.0556\n",
      "Epoch 6/150",
      "\n",
      "2041/2041 - 12s - loss: 3.5629 - accuracy: 0.0495 - val_loss: 3.4113 - val_accuracy: 0.0611\n",
      "Epoch 7/150",
      "\n",
      "2041/2041 - 13s - loss: 3.4394 - accuracy: 0.0652 - val_loss: 3.3205 - val_accuracy: 0.1056\n",
      "Epoch 8/150",
      "\n",
      "2041/2041 - 15s - loss: 3.3257 - accuracy: 0.0813 - val_loss: 3.2059 - val_accuracy: 0.1083\n",
      "Epoch 9/150",
      "\n",
      "2041/2041 - 13s - loss: 3.2481 - accuracy: 0.0892 - val_loss: 3.1564 - val_accuracy: 0.1028\n",
      "Epoch 10/150",
      "\n",
      "2041/2041 - 14s - loss: 3.2165 - accuracy: 0.0818 - val_loss: 3.0755 - val_accuracy: 0.1278\n",
      "Epoch 11/150",
      "\n",
      "2041/2041 - 13s - loss: 3.1436 - accuracy: 0.1093 - val_loss: 3.0049 - val_accuracy: 0.1444\n",
      "Epoch 12/150",
      "\n",
      "2041/2041 - 15s - loss: 3.0935 - accuracy: 0.1293 - val_loss: 2.8951 - val_accuracy: 0.1611\n",
      "Epoch 13/150",
      "\n",
      "2041/2041 - 16s - loss: 3.0171 - accuracy: 0.1352 - val_loss: 2.8203 - val_accuracy: 0.2194\n",
      "Epoch 14/150",
      "\n",
      "2041/2041 - 15s - loss: 2.9782 - accuracy: 0.1338 - val_loss: 2.8050 - val_accuracy: 0.2556\n",
      "Epoch 15/150",
      "\n",
      "2041/2041 - 13s - loss: 2.9354 - accuracy: 0.1602 - val_loss: 2.7484 - val_accuracy: 0.2639\n",
      "Epoch 16/150",
      "\n",
      "2041/2041 - 12s - loss: 2.8644 - accuracy: 0.1666 - val_loss: 2.7072 - val_accuracy: 0.2667\n",
      "Epoch 17/150",
      "\n",
      "2041/2041 - 12s - loss: 2.7908 - accuracy: 0.1911 - val_loss: 2.6203 - val_accuracy: 0.2972\n",
      "Epoch 18/150",
      "\n",
      "2041/2041 - 14s - loss: 2.7407 - accuracy: 0.1921 - val_loss: 2.5851 - val_accuracy: 0.2750\n",
      "Epoch 19/150",
      "\n",
      "2041/2041 - 13s - loss: 2.6898 - accuracy: 0.2166 - val_loss: 2.5359 - val_accuracy: 0.3083\n",
      "Epoch 20/150",
      "\n",
      "2041/2041 - 16s - loss: 2.6278 - accuracy: 0.2190 - val_loss: 2.4607 - val_accuracy: 0.3111\n",
      "Epoch 21/150",
      "\n",
      "2041/2041 - 14s - loss: 2.5480 - accuracy: 0.2450 - val_loss: 2.4255 - val_accuracy: 0.3111\n",
      "Epoch 22/150",
      "\n",
      "2041/2041 - 14s - loss: 2.5056 - accuracy: 0.2680 - val_loss: 2.3561 - val_accuracy: 0.3306\n",
      "Epoch 23/150",
      "\n",
      "2041/2041 - 15s - loss: 2.4595 - accuracy: 0.2626 - val_loss: 2.3035 - val_accuracy: 0.3556\n",
      "Epoch 24/150",
      "\n",
      "2041/2041 - 17s - loss: 2.3854 - accuracy: 0.2768 - val_loss: 2.1982 - val_accuracy: 0.3889\n",
      "Epoch 25/150",
      "\n",
      "2041/2041 - 16s - loss: 2.3215 - accuracy: 0.2994 - val_loss: 2.1616 - val_accuracy: 0.4306\n",
      "Epoch 26/150",
      "\n",
      "2041/2041 - 15s - loss: 2.3238 - accuracy: 0.2935 - val_loss: 2.1821 - val_accuracy: 0.4222\n",
      "Epoch 27/150",
      "\n",
      "2041/2041 - 14s - loss: 2.2290 - accuracy: 0.3425 - val_loss: 2.1236 - val_accuracy: 0.4056\n",
      "Epoch 28/150",
      "\n",
      "2041/2041 - 13s - loss: 2.1823 - accuracy: 0.3435 - val_loss: 2.0361 - val_accuracy: 0.4250\n",
      "Epoch 29/150",
      "\n",
      "2041/2041 - 13s - loss: 2.1216 - accuracy: 0.3626 - val_loss: 1.9826 - val_accuracy: 0.4306\n",
      "Epoch 30/150",
      "\n",
      "2041/2041 - 13s - loss: 2.0554 - accuracy: 0.3846 - val_loss: 1.9750 - val_accuracy: 0.4444\n",
      "Epoch 31/150",
      "\n",
      "2041/2041 - 13s - loss: 2.0299 - accuracy: 0.3900 - val_loss: 1.9824 - val_accuracy: 0.4472\n",
      "Epoch 32/150",
      "\n",
      "2041/2041 - 14s - loss: 1.9556 - accuracy: 0.4003 - val_loss: 1.9237 - val_accuracy: 0.4722\n",
      "Epoch 33/150",
      "\n",
      "2041/2041 - 13s - loss: 1.9265 - accuracy: 0.4179 - val_loss: 1.8741 - val_accuracy: 0.4472\n",
      "Epoch 34/150",
      "\n",
      "2041/2041 - 14s - loss: 1.9064 - accuracy: 0.4184 - val_loss: 1.8596 - val_accuracy: 0.4750\n",
      "Epoch 35/150",
      "\n",
      "2041/2041 - 13s - loss: 1.8267 - accuracy: 0.4351 - val_loss: 1.8394 - val_accuracy: 0.4583\n",
      "Epoch 36/150",
      "\n",
      "2041/2041 - 12s - loss: 1.7922 - accuracy: 0.4532 - val_loss: 1.8159 - val_accuracy: 0.4778\n",
      "Epoch 37/150",
      "\n",
      "2041/2041 - 12s - loss: 1.7779 - accuracy: 0.4532 - val_loss: 1.7566 - val_accuracy: 0.4917\n",
      "Epoch 38/150",
      "\n",
      "2041/2041 - 12s - loss: 1.7621 - accuracy: 0.4473 - val_loss: 1.7485 - val_accuracy: 0.4944\n",
      "Epoch 39/150",
      "\n",
      "2041/2041 - 12s - loss: 1.6732 - accuracy: 0.4806 - val_loss: 1.6908 - val_accuracy: 0.5000\n",
      "Epoch 40/150",
      "\n",
      "2041/2041 - 13s - loss: 1.7074 - accuracy: 0.4836 - val_loss: 1.7200 - val_accuracy: 0.5000\n",
      "Epoch 41/150",
      "\n",
      "2041/2041 - 13s - loss: 1.6491 - accuracy: 0.4885 - val_loss: 1.7136 - val_accuracy: 0.5028\n",
      "Epoch 42/150",
      "\n",
      "2041/2041 - 13s - loss: 1.6030 - accuracy: 0.5208 - val_loss: 1.7014 - val_accuracy: 0.5194\n",
      "Epoch 43/150",
      "\n",
      "2041/2041 - 14s - loss: 1.6028 - accuracy: 0.5198 - val_loss: 1.6523 - val_accuracy: 0.5306\n",
      "Epoch 44/150",
      "\n",
      "2041/2041 - 13s - loss: 1.5388 - accuracy: 0.5179 - val_loss: 1.6323 - val_accuracy: 0.5250\n",
      "Epoch 45/150",
      "\n",
      "2041/2041 - 14s - loss: 1.4971 - accuracy: 0.5434 - val_loss: 1.6342 - val_accuracy: 0.5333\n",
      "Epoch 46/150",
      "\n",
      "2041/2041 - 12s - loss: 1.5062 - accuracy: 0.5424 - val_loss: 1.6030 - val_accuracy: 0.5583\n",
      "Epoch 47/150",
      "\n",
      "2041/2041 - 13s - loss: 1.4876 - accuracy: 0.5355 - val_loss: 1.6268 - val_accuracy: 0.5500\n",
      "Epoch 48/150",
      "\n",
      "2041/2041 - 15s - loss: 1.4534 - accuracy: 0.5693 - val_loss: 1.5564 - val_accuracy: 0.5472\n",
      "Epoch 49/150",
      "\n",
      "2041/2041 - 12s - loss: 1.4507 - accuracy: 0.5497 - val_loss: 1.5331 - val_accuracy: 0.5694\n",
      "Epoch 50/150",
      "\n",
      "2041/2041 - 12s - loss: 1.3561 - accuracy: 0.5816 - val_loss: 1.5378 - val_accuracy: 0.5750\n",
      "Epoch 51/150",
      "\n",
      "2041/2041 - 14s - loss: 1.4162 - accuracy: 0.5728 - val_loss: 1.5834 - val_accuracy: 0.5528\n",
      "Epoch 52/150",
      "\n",
      "2041/2041 - 13s - loss: 1.3859 - accuracy: 0.5639 - val_loss: 1.6131 - val_accuracy: 0.5472\n",
      "Epoch 53/150",
      "\n",
      "2041/2041 - 15s - loss: 1.3842 - accuracy: 0.5777 - val_loss: 1.4882 - val_accuracy: 0.5833\n",
      "Epoch 54/150",
      "\n",
      "2041/2041 - 16s - loss: 1.3371 - accuracy: 0.6002 - val_loss: 1.5300 - val_accuracy: 0.5889\n",
      "Epoch 55/150",
      "\n",
      "2041/2041 - 13s - loss: 1.3080 - accuracy: 0.6056 - val_loss: 1.4967 - val_accuracy: 0.5750\n",
      "Epoch 56/150",
      "\n",
      "2041/2041 - 13s - loss: 1.3366 - accuracy: 0.5806 - val_loss: 1.4791 - val_accuracy: 0.5889\n",
      "Epoch 57/150",
      "\n",
      "2041/2041 - 15s - loss: 1.2551 - accuracy: 0.6041 - val_loss: 1.5225 - val_accuracy: 0.5694\n",
      "Epoch 58/150",
      "\n",
      "2041/2041 - 17s - loss: 1.2764 - accuracy: 0.6012 - val_loss: 1.4662 - val_accuracy: 0.5667\n",
      "Epoch 59/150",
      "\n",
      "2041/2041 - 15s - loss: 1.2445 - accuracy: 0.6164 - val_loss: 1.4534 - val_accuracy: 0.6083\n",
      "Epoch 60/150",
      "\n",
      "2041/2041 - 16s - loss: 1.2024 - accuracy: 0.6355 - val_loss: 1.4786 - val_accuracy: 0.6028\n",
      "Epoch 61/150",
      "\n",
      "2041/2041 - 12s - loss: 1.1715 - accuracy: 0.6262 - val_loss: 1.4689 - val_accuracy: 0.5750\n",
      "Epoch 62/150",
      "\n",
      "2041/2041 - 13s - loss: 1.1595 - accuracy: 0.6389 - val_loss: 1.4300 - val_accuracy: 0.5944\n",
      "Epoch 63/150",
      "\n",
      "2041/2041 - 13s - loss: 1.1665 - accuracy: 0.6335 - val_loss: 1.4039 - val_accuracy: 0.6139\n",
      "Epoch 64/150",
      "\n",
      "2041/2041 - 13s - loss: 1.1483 - accuracy: 0.6458 - val_loss: 1.4209 - val_accuracy: 0.6028\n",
      "Epoch 65/150",
      "\n",
      "2041/2041 - 14s - loss: 1.1396 - accuracy: 0.6423 - val_loss: 1.4190 - val_accuracy: 0.6083\n",
      "Epoch 66/150",
      "\n",
      "2041/2041 - 15s - loss: 1.1226 - accuracy: 0.6423 - val_loss: 1.4646 - val_accuracy: 0.5833\n",
      "Epoch 67/150",
      "\n",
      "2041/2041 - 14s - loss: 1.0865 - accuracy: 0.6663 - val_loss: 1.3978 - val_accuracy: 0.6306\n",
      "Epoch 68/150",
      "\n",
      "2041/2041 - 13s - loss: 1.0852 - accuracy: 0.6639 - val_loss: 1.5192 - val_accuracy: 0.6028\n",
      "Epoch 69/150",
      "\n",
      "2041/2041 - 15s - loss: 1.0979 - accuracy: 0.6551 - val_loss: 1.4649 - val_accuracy: 0.6139\n",
      "Epoch 70/150",
      "\n",
      "2041/2041 - 17s - loss: 1.0927 - accuracy: 0.6678 - val_loss: 1.4127 - val_accuracy: 0.6278\n",
      "Epoch 71/150",
      "\n",
      "2041/2041 - 15s - loss: 1.0615 - accuracy: 0.6766 - val_loss: 1.4505 - val_accuracy: 0.6222\n",
      "Epoch 72/150",
      "\n",
      "2041/2041 - 15s - loss: 1.0458 - accuracy: 0.6570 - val_loss: 1.4380 - val_accuracy: 0.6167\n",
      "Epoch 73/150",
      "\n",
      "2041/2041 - 12s - loss: 1.0041 - accuracy: 0.6913 - val_loss: 1.4226 - val_accuracy: 0.6167\n",
      "Epoch 74/150",
      "\n",
      "2041/2041 - 18s - loss: 1.0543 - accuracy: 0.6747 - val_loss: 1.4237 - val_accuracy: 0.6278\n",
      "Epoch 75/150",
      "\n",
      "2041/2041 - 17s - loss: 1.0019 - accuracy: 0.6908 - val_loss: 1.3755 - val_accuracy: 0.6222\n",
      "Epoch 76/150",
      "\n",
      "2041/2041 - 16s - loss: 0.9616 - accuracy: 0.7006 - val_loss: 1.3560 - val_accuracy: 0.6250\n",
      "Epoch 77/150",
      "\n",
      "2041/2041 - 13s - loss: 1.0417 - accuracy: 0.6796 - val_loss: 1.4763 - val_accuracy: 0.6139\n",
      "Epoch 78/150",
      "\n",
      "2041/2041 - 15s - loss: 0.9445 - accuracy: 0.7055 - val_loss: 1.3439 - val_accuracy: 0.6500\n",
      "Epoch 79/150",
      "\n",
      "2041/2041 - 14s - loss: 0.9384 - accuracy: 0.7060 - val_loss: 1.4304 - val_accuracy: 0.6222\n",
      "Epoch 80/150",
      "\n",
      "2041/2041 - 15s - loss: 0.9320 - accuracy: 0.7095 - val_loss: 1.3982 - val_accuracy: 0.6167\n",
      "Epoch 81/150",
      "\n",
      "2041/2041 - 12s - loss: 0.9321 - accuracy: 0.7031 - val_loss: 1.3417 - val_accuracy: 0.6472\n",
      "Epoch 82/150",
      "\n",
      "2041/2041 - 12s - loss: 0.8706 - accuracy: 0.7330 - val_loss: 1.4296 - val_accuracy: 0.6250\n",
      "Epoch 83/150",
      "\n",
      "2041/2041 - 13s - loss: 0.9239 - accuracy: 0.7129 - val_loss: 1.3579 - val_accuracy: 0.6194\n",
      "Epoch 84/150",
      "\n",
      "2041/2041 - 13s - loss: 0.8787 - accuracy: 0.7168 - val_loss: 1.3308 - val_accuracy: 0.6389\n",
      "Epoch 85/150",
      "\n",
      "2041/2041 - 15s - loss: 0.8693 - accuracy: 0.7393 - val_loss: 1.3730 - val_accuracy: 0.6556\n",
      "Epoch 86/150",
      "\n",
      "2041/2041 - 13s - loss: 0.9139 - accuracy: 0.7207 - val_loss: 1.3495 - val_accuracy: 0.6278\n",
      "Epoch 87/150",
      "\n",
      "2041/2041 - 15s - loss: 0.8516 - accuracy: 0.7428 - val_loss: 1.4160 - val_accuracy: 0.6306\n",
      "Epoch 88/150",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "MODEL = create_cnn_model()\n",
    "MODEL.summary()\n",
    "history = MODEL.fit(train_X, train_Y, batch_size=32, epochs=150, validation_data=(test_X, test_Y), verbose=2, callbacks=[TENSORBOARD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "MODEL.save(f'{MODEL_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# MODEL = tf.keras.models.load_model(f'{MODEL_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert model to TensorFlow Lite format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"converter = tf.lite.TFLiteConverter.from_keras_model(MODEL)\n",
    "tflite_model = converter.convert()\n",
    "open(\"converted_model.tflite\", \"wb\").write(tflite_model)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "CLASS_INDEX = dict(zip([np.argmax(x) for x in encoded_labels], LABELS.flatten()))\n",
    "# CLASS_INDEX = dict(sorted(CLASS_INDEX.items()))\n",
    "LABELS = LABELS.flatten()\n",
    "IMAGE_IDs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Plot of Predicted Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "results = {cls: [] for cls in LABELS}\n",
    "\n",
    "# iterate over each image in test_sample\n",
    "# get the model's class prediction of the image\n",
    "for num, data in enumerate(test_data):\n",
    "    data[0] = data[0] / 255.0\n",
    "    img_data = data[0]\n",
    "    img_num = data[1]\n",
    "    y = fig.add_subplot(6, 6, num + 1)\n",
    "    orig = img_data\n",
    "    data = img_data.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "    model_out = MODEL.predict([data]).flatten()\n",
    "    index = np.argmax(model_out)\n",
    "    # generate output dictionary\n",
    "    results = {LABELS[i]: results.get(LABELS[i]) + [model_out[i]] for i in range(NUM_CLASSES)}\n",
    "    IMAGE_IDs.append(img_num)\n",
    "    \n",
    "    # cross-reference the predicted class-index to its class-label (for each test image)\n",
    "    class_label = CLASS_INDEX.get(index, 'Invalid class!')\n",
    "    print(f\"Image ID: {img_num}\\t | Prediction: {class_label}\")\n",
    "\n",
    "    y.imshow(orig, cmap='gray')\n",
    "    plt.title(f'{img_num}: {class_label}')\n",
    "    y.axes.get_xaxis().set_visible(False)\n",
    "    y.axes.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "# plt.savefig('Class Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "imgs = [img.split('.')[0] for img in next(os.walk(ALIGNED_TEST_DIR))[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "### Tabulated Prediction Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creates a HeatMap using the seaborn library\n",
    "cm = sns.light_palette(\"red\", as_cmap=True)\n",
    "df = pd.DataFrame.from_dict(results, orient='index', columns=imgs)\n",
    "df.style.\\\n",
    "    format(\"{:.2%}\").\\\n",
    "    set_caption('Confidence Values')\\\n",
    "    .background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Re-structures the results dictionary so that each class_label points to another dictionary {k, v}\n",
    "where k = the Image_Id number and v = the confidence value\n",
    "\"\"\"\n",
    "\n",
    "def gen_results(results):\n",
    "    my_dict = {}\n",
    "    for cls in LABELS:\n",
    "        probs = iter(results[cls])\n",
    "        my_dict.update({cls: {}})\n",
    "        for k in IMAGE_IDs:\n",
    "            my_dict[cls][int(k)] = next(probs)\n",
    "\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_top5(results, ID=1):\n",
    "    results = gen_results(results)\n",
    "    probs = np.array([(results[k][ID]) for k in results])\n",
    "    # print(f'Reverse: {(-probs).argsort()} - {sorted(probs, reverse=True)}')\n",
    "    indices = (-probs).argsort()[:5] # sorts probabilities (largest - smallest) + returns their corresponding array indices\n",
    "    top_5 = [CLASS_INDEX.get(i) for i in indices]\n",
    "    return top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Image_ID = 7\n",
    "TOP_5 = get_top5(results, Image_ID)\n",
    "TOP_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_overall_accuracy(results):\n",
    "    i = 0\n",
    "    num_correct = 0\n",
    "    total = len(test_data) # total number of images\n",
    "    keys = results.keys()\n",
    "    class_labels = []\n",
    "    \n",
    "    for ID in IMAGE_IDs: # loop through each image ID\n",
    "        predictions = []\n",
    "        for key in list(keys): # for each model in the results dictionary\n",
    "            prob = results[key].get(ID)\n",
    "            predictions.append(prob)\n",
    "        max_index = np.argmax(predictions) # max index\n",
    "        label = CLASS_INDEX.get(max_index, 'Invalid class!')\n",
    "        class_labels.append(label)\n",
    "    \n",
    "    for img in os.listdir(ALIGNED_TEST_DIR):\n",
    "        img = img.split('.')[0].strip() # gets the class name of the image file\n",
    "        if img == class_labels[i]:\n",
    "            num_correct += 1\n",
    "            # print(f\"Image name: {img} - predicted label: {class_labels[i]}\")\n",
    "        print(f\"Image name: {img} - predicted label: {class_labels[i]}\")\n",
    "        i += 1 \n",
    "        \n",
    "        \n",
    "    accuracy = round((num_correct / total) * 100, 2)\n",
    "    return f'{accuracy}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "get_overall_accuracy(gen_results(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow-2.0)",
   "language": "python",
   "name": "tensorflow-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}