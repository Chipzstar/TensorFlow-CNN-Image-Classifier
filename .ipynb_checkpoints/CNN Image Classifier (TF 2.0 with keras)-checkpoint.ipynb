{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
      "\u001b[K     |████████████████████████████████| 86.3MB 152kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.0.8)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting google-pasta>=0.1.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 2.5MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Collecting numpy<2.0,>=1.16.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/ab/43e678759326f728de861edbef34b8e2ad1b1490505f20e0d1f0716c3bf4/numpy-1.17.4-cp36-cp36m-manylinux1_x86_64.whl (20.0MB)\n",
      "\u001b[K     |████████████████████████████████| 20.0MB 41.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 40.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 41.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 13.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (0.8.0)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (3.7.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (42.0.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting requests<3,>=2.21.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 11.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/cb/786dc53d93494784935a62947643b48250b84a882474e714f9af5e1a1928/google_auth-1.7.1-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 1.1MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.23)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.9.11)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 54.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.4.2)\n",
      "Collecting cachetools<3.2,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Collecting oauthlib>=3.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 55.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.8)\n",
      "Building wheels for collected packages: gast, opt-einsum, wrapt\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=6588 sha256=6d706ce7c7327257f5f8dba00666642cd7abad6077ae9a10cb56c37afa7a1ab4\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for opt-einsum (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for opt-einsum: filename=opt_einsum-3.1.0-cp36-none-any.whl size=60860 sha256=0dc5a2a5621cdde06feaece1addd7fec2da91ceda38ec330ab0fd87f5076b1b6\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/2c/b1/94/43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.11.2-cp36-cp36m-linux_x86_64.whl size=65914 sha256=3174f56ff20908a90d2ddc8a7abce09a76943d2b179899a32beee163a6ca8af1\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
      "Successfully built gast opt-einsum wrapt\n",
      "\u001b[31mERROR: tensorflow-serving-api 1.13.0 has requirement tensorflow<2,>=1.2.0, but you'll have tensorflow 2.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: sagemaker 1.43.4.post1 has requirement requests<2.21,>=2.20.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: docker-compose 1.24.1 has requirement requests!=2.11.0,!=2.12.2,!=2.18.0,<2.21,>=2.6.1, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: gast, google-pasta, numpy, oauthlib, requests, requests-oauthlib, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, tensorboard, tensorflow-estimator, opt-einsum, wrapt, tensorflow\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.6 are installed in '/home/ec2-user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/home/ec2-user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/home/ec2-user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/ec2-user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed cachetools-3.1.1 gast-0.2.2 google-auth-1.7.1 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 numpy-1.17.4 oauthlib-3.1.0 opt-einsum-3.1.0 pyasn1-modules-0.2.7 requests-2.22.0 requests-oauthlib-1.3.0 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1 wrapt-1.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import PIL\n",
    "import boto3\n",
    "from collections import Counter\n",
    "from random import shuffle, randint, seed\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV version: 4.1.2\n",
      "Tensorflow version: 2.0.0\n",
      "Pillow version: 5.2.0\n"
     ]
    }
   ],
   "source": [
    "print(f'OpenCV version: {cv2.__version__}')\n",
    "print(f'Tensorflow version: {tf.__version__}')\n",
    "print(f'Pillow version: {PIL.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATE = datetime.date(2019, 11, 21).strftime('%d-%b-%Y')\n",
    "IMG_SIZE = 100\n",
    "LR = 1e-3\n",
    "PATH = 'uploads/'\n",
    "MODEL_PATH = f'models/{DATE}/'\n",
    "MODEL_NAME = 'ImageClassifier-keras-5-Conv-Layer-{}.model'.format(int(time.time()))\n",
    "IMG_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Bucket Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acl',\n",
       " 'Cors',\n",
       " 'Lifecycle',\n",
       " 'LifecycleConfiguration',\n",
       " 'Logging',\n",
       " 'Notification',\n",
       " 'Object',\n",
       " 'Policy',\n",
       " 'RequestPayment',\n",
       " 'Tagging',\n",
       " 'Versioning',\n",
       " 'Website']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('pornilarity-bucket170933-production')\n",
    "bucket.get_available_subresources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public/512dd806-d9b7-4526-905d-5cb7fd374aa8.jpg\n",
      "public/e33dd087-5b40-4aef-95f7-6387fc066393.jpg\n",
      "public/test.jpg\n"
     ]
    }
   ],
   "source": [
    "for key in bucket.objects.all():\n",
    "    name = key.key.split('/')[1]\n",
    "    print(key.key)\n",
    "    bucket.download_file(key.key, f'uploads/{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images():\n",
    "    test_images = []\n",
    "    for img in os.listdir(PATH):\n",
    "        img_name = str(img) \n",
    "        full_path = os.path.join(PATH, img)  # full path of the image\n",
    "        print(full_path)\n",
    "        # feature extraction\n",
    "        try:\n",
    "            img = cv2.imread(full_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            img = tf.cast(img, tf.float32)  # change data type of image to float32\n",
    "            test_images.append(img)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "    return np.array(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploads/test_img.jpg\n",
      "uploads/.ipynb_checkpoints\n",
      "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
      "\n",
      "uploads/e33dd087-5b40-4aef-95f7-6387fc066393.jpg\n",
      "uploads/512dd806-d9b7-4526-905d-5cb7fd374aa8.jpg\n"
     ]
    }
   ],
   "source": [
    "test_images = process_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 1)\n",
      "(100, 100, 1)\n",
      "(100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "test_images = np.array([i for i in test_images]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "for img in test_images:\n",
    "    print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 98, 98, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 98, 98, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 96, 96, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 46, 46, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 23, 23, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 21, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 10, 10, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 95)                24415     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 95)                0         \n",
      "=================================================================\n",
      "Total params: 1,472,255\n",
      "Trainable params: 1,471,295\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MODEL = tf.keras.models.load_model(f'{MODEL_PATH}')\n",
    "MODEL.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read in Labels Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('labels.txt', 'r') as file:\n",
    "    labels = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(test_images):\n",
    "    predictions = MODEL.predict(test_images)\n",
    "    predicted_labels = []\n",
    "    for pred in predictions:\n",
    "        index = np.argmax(pred)\n",
    "        label = labels[index]\n",
    "        predicted_labels.append(label)\n",
    "\n",
    "    return np.array(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Christy Mack', 'Christy Mack', 'Dana DeArmond'], dtype='<U13')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = get_predictions(test_images)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"predictions.txt\", predicted_labels, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_data = np.load('test_data.npy', allow_pickle=True)\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "results = {cls: [] for cls in LABELS}\n",
    "\n",
    "# iterate over each image in test_sample\n",
    "# get the model's class prediction of the image\n",
    "for num, data in enumerate(test_data):\n",
    "    data[0] = data[0] / 255.0\n",
    "    img_data = data[0]\n",
    "    img_num = data[1]\n",
    "    y = fig.add_subplot(6, 6, num + 1)\n",
    "    orig = img_data\n",
    "    data = img_data.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "    model_out = MODEL.predict([data]).flatten()\n",
    "    index = np.argmax(model_out)\n",
    "    # generate output dictionary\n",
    "    results = {LABELS[i]: results.get(LABELS[i]) + [model_out[i]] for i in range(NUM_CLASSES)}\n",
    "    IMAGE_IDs.append(img_num)\n",
    "    \n",
    "    # cross-reference the predicted class-index to its class-label (for each test image)\n",
    "    class_label = CLASS_INDEX.get(index, 'Invalid class!')\n",
    "    print(f\"Image ID: {img_num}\\t | Prediction: {class_label}\")\n",
    "\n",
    "    y.imshow(orig, cmap='gray')\n",
    "    plt.title(f'{img_num}: {class_label}')\n",
    "    y.axes.get_xaxis().set_visible(False)\n",
    "    y.axes.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "# plt.savefig('Class Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "IMAGES_NAMES = [img.split('.')[0] for img in next(os.walk(ALIGNED_TEST_DIR))[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Top 5 results for Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Re-structures the results dictionary so that each class_label points to another dictionary {k, v}\n",
    "where k = the Image_Id number and v = the confidence value\n",
    "\"\"\"\n",
    "\n",
    "def gen_results(results):\n",
    "    my_dict = {}\n",
    "    for cls in LABELS:\n",
    "        probs = iter(results[cls])\n",
    "        my_dict.update({cls: {}})\n",
    "        for k in IMAGE_IDs:\n",
    "            my_dict[cls][int(k)] = next(probs)\n",
    "\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_top5(results, ID=1):\n",
    "    results = gen_results(results)\n",
    "    probs = np.array([(results[k][ID]) for k in results])\n",
    "    # print(f'Reverse: {(-probs).argsort()} - {sorted(probs, reverse=True)}')\n",
    "    indices = (-probs).argsort()[:5] # sorts probabilities (largest - smallest) + returns their corresponding array indices\n",
    "    top_5 = [CLASS_INDEX.get(i) for i in indices]\n",
    "    return top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Image_ID = 3\n",
    "TOP_5 = get_top5(results, Image_ID)\n",
    "TOP_5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
